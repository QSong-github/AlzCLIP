{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:53:51.578472Z",
     "start_time": "2024-06-28T06:53:51.551587Z"
    }
   },
   "source": [
    "from Uni.model import CLIPModel_simple\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "model_path = \"./save/best128.pt\"\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='BRAIN CLIP')\n",
    "    \n",
    "    parser.add_argument('--save_path', type=str, default='./save', help='')\n",
    "    parser.add_argument('--embedding_dim', type=int, default=256, help='')\n",
    "    parser.add_argument('--projection_dim', type=int, default=128, help='')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='')\n",
    "    parser.add_argument('--temperature', type=float, default=1.0, help='')\n",
    "    parser.add_argument('--vote', default=True, type=bool, help='')\n",
    "    parser.add_argument('--topk', type=int, default=3, help='')\n",
    "    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='')\n",
    "    \n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "model = CLIPModel_simple(args).to(args.device)\n",
    "state_dict = torch.load(model_path)\n",
    "new_state_dict = {}\n",
    "for key in state_dict.keys():\n",
    "    new_key = key.replace('module.', '')  # remove the prefix 'module.'\n",
    "    new_key = new_key.replace('well', 'spot')  # for compatibility with prior naming\n",
    "    new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Uni/save/best128.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel_simple(\n",
       "  (image_encoder): ImageEncoder_linear(\n",
       "    (model): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (snabel_encoder): SnabelEncoder_linear(\n",
       "    (model): Linear(in_features=234, out_features=256, bias=True)\n",
       "  )\n",
       "  (image_projection): EmbedHead(\n",
       "    (projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (snabel_projection): EmbedHead(\n",
       "    (projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:53:55.979039Z",
     "start_time": "2024-06-28T06:53:55.972684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def get_img_embeddings(img, model):\n",
    "    model.eval()\n",
    "    image_features = model.image_encoder(img.cuda())\n",
    "    image_embeddings = model.image_projection(image_features)\n",
    "\n",
    "    return image_embeddings\n",
    "\n",
    "\n",
    "def get_snp_embeddings(snp, model):\n",
    "    model.eval()\n",
    "    snp_features = model.snabel_encoder(snp.cuda())\n",
    "    snp_embeddings = model.snabel_projection(snp_features)\n",
    "\n",
    "    return snp_embeddings"
   ],
   "id": "341f292f1f825e06",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:53:58.514311Z",
     "start_time": "2024-06-28T06:53:58.480280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# make your SNP or IMG as input\n",
    "snp = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
    "        1., 0., 0., 2., 1., 0., 0., 0., 0., 2., 1., 1., 2., 0., 1., 1., 0., 0.,\n",
    "        2., 0., 2., 1., 0., 0., 2., 1., 1., 0., 0., 2., 0., 2., 2., 2., 1., 1.,\n",
    "        0., 1., 2., 2., 2., 0., 2., 2., 0., 0., 2., 2., 1., 0., 1., 0., 0., 1.,\n",
    "        1., 2., 1., 0., 2., 2., 1., 0., 0., 2., 2., 2., 1., 0., 1., 0., 2., 2.,\n",
    "        0., 1., 2., 2., 2., 0., 2., 2., 1., 0., 2., 0., 0., 0., 1., 2., 2., 0.,\n",
    "        2., 2., 2., 0., 0., 2., 1., 2., 2., 0., 0., 2., 0., 0., 1., 1., 1., 0.,\n",
    "        1., 2., 2., 0., 1., 1., 1., 0., 2., 0., 1., 1., 0., 2., 2., 0., 1., 2.,\n",
    "        0., 2., 0., 0., 2., 0., 2., 2., 1., 0., 2., 0., 1., 2., 0., 0., 0., 0.,\n",
    "        1., 1., 0., 2., 1., 0., 1., 2., 1., 2., 2., 1., 1., 0., 2., 0., 0., 0.,\n",
    "        2., 0., 2., 1., 1., 1., 2., 1., 0., 1., 2., 0., 2., 0., 2., 2., 0., 0.,\n",
    "        1., 1., 2., 0., 2., 0., 2., 2., 0., 2., 1., 0., 2., 2., 2., 0., 0., 2.,\n",
    "        2., 2., 1., 0., 0., 2., 0., 1., 0., 2., 1., 0., 0., 0., 0., 0., 2., 0.])\n",
    "\n",
    "if len(snp)>234:\n",
    "    snp = snp[:234]\n",
    "while len(snp)<234:\n",
    "    snp.append(99)  # 99 as pad\n",
    "    \n",
    "snp = snp.unsqueeze(0)\n",
    "\n",
    "img = torch.tensor([  670.2590,    87.7632,   226.5341,   436.2836,   134.7943,   186.3737,\n",
    "         -172.9206, -1513.7437,  -193.6193,    28.0608,    11.5967,   449.3401,\n",
    "          449.2867,   208.9356,   -61.2091,  -120.9889,   267.1925,  -126.9980,\n",
    "          954.8094,  -609.8949,   786.1028,  -176.6902,   104.8324,    60.3755,\n",
    "          425.5132,  -261.9039,   282.5353,  -122.8213,   756.0443,  -270.4344,\n",
    "          -34.8492,  -834.0070,   122.5162,   407.1969,  1160.2214,  -428.9731,\n",
    "         -631.9886,    97.9889,  -106.9545,   -36.9184,   867.3173,  -318.7118,\n",
    "         -320.3081,  -320.8904,   845.5027,  1566.8265,   283.6511,   187.8840,\n",
    "         -336.6803,   -81.8993,  -638.3338,   477.3666,   541.8198,   -50.5883,\n",
    "         -202.5780,  -486.9493,   142.1280,    18.4567,  -132.8069,   -79.4203,\n",
    "         -430.0923,   180.3173,  1016.8868,  -297.6619,   284.1148,   -17.6610,\n",
    "          539.2893,   186.5835,    91.3026,   -36.7723,   178.6299,  -294.6639,\n",
    "         -175.4978,   413.0392,   148.5926,  -548.8325,   -92.3212,   179.4273,\n",
    "        -2874.2920,  3197.5642,  -230.1659,  -338.6367,    58.8648,  -307.3961,\n",
    "         -402.1805,  -325.3324,   364.9382,   288.9287,   516.7051,  -125.3412,\n",
    "           43.9312,   829.7002,   105.6240,   -15.1332,   896.2142,   845.3306,\n",
    "         -689.6951,  -328.3130,   361.1234,   444.5173,   -88.8024,   362.8145,\n",
    "          263.4659,    64.9619,  -232.2993,   -25.8410,   231.1041,   191.6732,\n",
    "          268.9848,   849.3340,   378.3295,  -388.5266, -1046.9761,  -499.5015,\n",
    "         -439.0554,  -323.1657,  -202.1560,   -30.4386,  -315.0648,  -613.0076,\n",
    "          190.9714,  1115.7202,  -494.9961,   387.6808,   629.5889,     3.4351,\n",
    "          465.5319,  -177.0818])\n",
    "\n",
    "img = img.unsqueeze(0)\n",
    "snp.shape\n",
    "img.shape"
   ],
   "id": "f75becd6b30802f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:54:03.354694Z",
     "start_time": "2024-06-28T06:54:03.346610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_embeddings = get_img_embeddings(img, model)\n",
    "snp_embeddings = get_snp_embeddings(snp, model)\n",
    "img_embeddings.shape\n",
    "# snp_embeddings.shape"
   ],
   "id": "eb6d3f5bb6c514c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:55:03.991404Z",
     "start_time": "2024-06-28T06:55:03.981707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# query = np.load(\"./embeddings/img_embeddings_query.npy\")\n",
    "# query = snp_embeddings\n",
    "query = img_embeddings\n",
    "refer = np.load(\"./embeddings/img_embeddings_refer.npy\")\n",
    "label_refer = np.load(\"./embeddings/labels_refer.npy\")\n",
    "# label_query = np.load(\"./embeddings/labels_query.npy\")\n",
    "if refer.shape[1]!=128:\n",
    "    refer = refer.T\n",
    "    \n",
    "query.shape\n",
    "refer.shape"
   ],
   "id": "5943895d8252e484",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7671, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:55:05.583876Z",
     "start_time": "2024-06-28T06:55:05.576789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "def find_matches(refer_embeddings, query_embeddings, topk):\n",
    "    # find the closest matches\n",
    "    refer_embeddings = torch.tensor(refer_embeddings).cuda()\n",
    "    query_embeddings = torch.tensor(query_embeddings).cuda()\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=-1)\n",
    "    refer_embeddings = F.normalize(refer_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = query_embeddings @ refer_embeddings.T\n",
    "    print(dot_similarity.shape)\n",
    "    _, indices = torch.topk(dot_similarity.squeeze(0), k=topk)\n",
    "\n",
    "    return indices.cpu().numpy()"
   ],
   "id": "b2c9d6cd6c9072ea",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T07:04:56.745299Z",
     "start_time": "2024-06-28T07:04:56.729585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = find_matches(refer,query,topk=args.topk)\n",
    "print(index)\n",
    "if args.vote==True:\n",
    "    # The voting mechanism compares the returned index samples\n",
    "    # and their corresponding labels to make a vote.\n",
    "    print('voting')\n",
    "    if np.sum(label_refer[index] == 0)>np.sum(label_refer[index] == 1):\n",
    "        pred = 0\n",
    "        print('Not CN (AD or EMCI or LMCI or MCI or SMC or Patient)')\n",
    "    else:\n",
    "        pred = 1\n",
    "        print('CN')\n",
    "\n",
    "else:\n",
    "    # Directly select the label of the index sample with the highest probability as the result.\n",
    "    print('Max')\n",
    "    pred = label_refer[index[0]]\n",
    "    if pred == 0.0:\n",
    "        print('Not CN (AD or EMCI or LMCI or MCI or SMC or Patient)')\n",
    "    elif pred == 1.0:\n",
    "        print('CN')"
   ],
   "id": "13dfd1b527facdf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7671])\n",
      "[4512 3898  914]\n",
      "voting\n",
      "Not CN (AD or EMCI or LMCI or MCI or SMC or Patient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
